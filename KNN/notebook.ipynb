{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Reccomendation Algorithm\n",
    "\n",
    "I decided to use the PyNNDescent, which is based on the NN Descent algorithm. I decided to use an approximate nearest neighbors instead of true KNN as I calculated the time to find the predictions on the full data set to be around 2.5 hours. This wasn't feasable as lots of iterations needed to be ran. I used NN Descent as it is a scalable algorithm with relatively low overhead, and had a highly recommended python library to go along with it. I would have used HNSW, however the memory overhead had me worried as memory consumption was already a significant bottleneck.\n",
    "\n",
    "With NN Descent the total time to fit and predict on the full dataset is about 10 minutes, which is much more managable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from math import ceil, floor, sqrt\n",
    "from average import AverageRating\n",
    "from tqdm import tqdm\n",
    "from pynndescent.pynndescent_ import PyNNDescentTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample):\n",
    "    folder_path = \"C:\\\\Users\\\\holcombea\\\\grad\\\\Recommendation Systems\\\\movie-lens\"\n",
    "    ratings_path = folder_path + \"\\\\ratings.csv\"\n",
    "\n",
    "    if sample:\n",
    "        ratings_path = \"C:\\\\Users\\\\holcombea\\\\grad\\\\Recommendation Systems\\\\KNN\\\\sampe_data.csv\"\n",
    "        pass\n",
    "    ratings = pd.read_csv(ratings_path)\n",
    "    if not sample:\n",
    "        original_movie_ids = set(ratings['movieId'])\n",
    "        movie_id_map = {original: new for new, original in enumerate(original_movie_ids)}\n",
    "        ratings['movieId'] = ratings['movieId'].map(movie_id_map)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = load_data(sample=False)\n",
    "data = csr_matrix((ratings['rating'], (ratings['userId'], ratings['movieId'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162542, 59047)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNRating:\n",
    "    def __init__(self, k, metric: str):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.neighbors = PyNNDescentTransformer(n_neighbors=self.k, metric=self.metric)\n",
    "        self.training_ratings = None\n",
    "\n",
    "\n",
    "    def fit(self, training_ratings):\n",
    "        max_k = training_ratings.shape[0]\n",
    "        if self.k > max_k:\n",
    "            print(f\"k({self.k}) is greater than total samples, changing k to the number of samples({max_k})\")\n",
    "            self.k = max_k\n",
    "        self.neighbors = PyNNDescentTransformer(n_neighbors=self.k, metric=self.metric)\n",
    "        self.neighbors.fit(training_ratings)\n",
    "        self.training_ratings = training_ratings\n",
    "        \n",
    "\n",
    "    def predict(self, user_ratings):\n",
    "        if self.training_ratings is None:\n",
    "            raise Exception(\"Must fit before predicting\")\n",
    "        distances = self.neighbors.transform(user_ratings)\n",
    "        all_distances = distances.nonzero()[1]\n",
    "        neighbor_indices = np.array_split(all_distances, ceil(all_distances.shape[0]/self.k))\n",
    "        averageRating = AverageRating()\n",
    "        predictions = list()\n",
    "        for item in neighbor_indices:\n",
    "            averageRating.fit(self.training_ratings[item])\n",
    "            predictions.append(averageRating.predictions)\n",
    "        return vstack(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    \"\"\"\n",
    "    Class for splitting data and applying idf transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, train_size=0.8, idf=False, norm=None):\n",
    "        self.raw_data = data\n",
    "        self.train_size = train_size\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        self.seen = None\n",
    "        self.unseen = None\n",
    "        if idf:\n",
    "            idf_transformer = TfidfTransformer(norm=norm, use_idf=True)\n",
    "            self.raw_data = idf_transformer.fit_transform(self.raw_data)\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\"\n",
    "        Splits data into train and test. Also splits test data into seen and unseen groups.\n",
    "        \"\"\"\n",
    "        self.train, self.test = DataPrep.train_test_split(self.raw_data, train_size=self.train_size)\n",
    "        self.seen, self.unseen = DataPrep.user_split(self.test)\n",
    "        return self.train, self.seen, self.unseen\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_test_split(data, train_size):\n",
    "        train, test = sklearn_train_test_split(data, train_size=train_size)\n",
    "        return train, test\n",
    "\n",
    "    @staticmethod\n",
    "    def user_split(user_item_matrix: csr_matrix, split=0.8):\n",
    "        seen_data = np.array([])\n",
    "        seen_indices = np.array([])\n",
    "        seen_indptr = np.array([0])\n",
    "\n",
    "        unseen_data = np.array([])\n",
    "        unseen_indices = np.array([])\n",
    "        unseen_indptr = np.array([0])\n",
    "\n",
    "        for i in range(len(user_item_matrix.indptr.copy()) - 1):\n",
    "            row_start = user_item_matrix.indptr[i]\n",
    "            row_end   = user_item_matrix.indptr[i+1] \n",
    "            sample_size = floor((row_end - row_start) * split)\n",
    "            if sample_size == 0: #ensures something is in the test data\n",
    "                sample_size += 1\n",
    "\n",
    "            row_indices = user_item_matrix.indices[row_start: row_end]\n",
    "            row_data = user_item_matrix.data[row_start: row_end]\n",
    "\n",
    "\n",
    "            data_idx = np.arange(len(row_data))\n",
    "            seen_idx = np.random.choice(data_idx, size=sample_size, replace=False)\n",
    "            unseen_idx = np.setdiff1d(data_idx, seen_idx)\n",
    "            \n",
    "            #appending data to matrices\n",
    "            seen_data = np.append(seen_data, row_data[seen_idx])\n",
    "            unseen_data = np.append(unseen_data, row_data[unseen_idx])\n",
    "\n",
    "            #appending indices\n",
    "            seen_indices = np.append(seen_indices, row_indices[seen_idx])\n",
    "            unseen_indices = np.append(unseen_indices, row_indices[unseen_idx])\n",
    "\n",
    "            seen_indptr = np.append(seen_indptr, [seen_indptr[-1] + len(seen_idx)])\n",
    "            unseen_indptr = np.append(unseen_indptr, [unseen_indptr[-1] + len(unseen_idx)])\n",
    "        return csr_matrix((seen_data, seen_indices, seen_indptr), dtype=np.float32), csr_matrix((unseen_data, unseen_indices, unseen_indptr),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(train, seen, unseen, k, metric,batch_size):\n",
    "    model = KNNRating(k=k, metric=metric)\n",
    "    model.fit(train)\n",
    "\n",
    "    batched_input = list()\n",
    "    if batch_size > seen.shape[0]:\n",
    "        batched_input.append(seen)\n",
    "    else:\n",
    "        for i in range(0, seen.shape[0], batch_size):\n",
    "            batched_input.append(seen[i:i+batch_size])\n",
    "    predictions = list()\n",
    "    for batch in tqdm(batched_input):\n",
    "        predictions.append(model.predict(batch))\n",
    "    y_pred = vstack(predictions)\n",
    "    return y_pred, unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(predicted, true, rmse=True, pearson=True, sparsity=True, set_diff=True, debug=True):\n",
    "    nonzeros = true.nonzero()\n",
    "    y_true = np.array(true[nonzeros], dtype=np.float32)[0]\n",
    "    y_pred = np.array(predicted[nonzeros], dtype=np.float32)[0]\n",
    "    outputs = dict()\n",
    "    if rmse:\n",
    "        rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        outputs['rmse'] = rmse\n",
    "        if debug:\n",
    "            print(f\"rmse: {rmse}\")\n",
    "    if pearson:\n",
    "        r_squared = pearsonr(y_true, y_pred)[1] ** 2\n",
    "        outputs['r_squared'] = r_squared\n",
    "        if debug:\n",
    "            print(f\"r2 score: {r_squared}\")\n",
    "    if set_diff:\n",
    "        #batching set diff\n",
    "        batch_size=400\n",
    "        pred_batched = list()\n",
    "        true_batched = list()\n",
    "        for i in range(0, true.shape[0], batch_size):\n",
    "            pred_batched.append(predicted[i:i+batch_size])\n",
    "            true_batched.append(true[i:i+batch_size])\n",
    "        pair_in_both = 0\n",
    "        pair_in_true = 0\n",
    "        for pred_batch, true_batch in tqdm(zip(pred_batched, true_batched)):\n",
    "            pred_users, pred_movies = pred_batch.nonzero()\n",
    "            pred_pairs = set(zip(pred_users,pred_movies))\n",
    "            true_users, true_movies = true_batch.nonzero()\n",
    "            true_pairs = set(zip(true_users, true_movies))\n",
    "\n",
    "            pair_in_both += len(true_pairs.intersection(pred_pairs))\n",
    "            pair_in_true += len(true_pairs)\n",
    "\n",
    "        non_zero_ratings = pair_in_both / pair_in_true\n",
    "        outputs['non_zero_ratings'] = non_zero_ratings\n",
    "        if debug:\n",
    "            print(f\"Fraction of user-movie pairs with non-zero predicted ratings: {non_zero_ratings}\")\n",
    "    if sparsity:\n",
    "        sparsity = predicted.getnnz() / (predicted.shape[0] * predicted.shape[1])\n",
    "        outputs['sparsity'] = predicted.getnnz() / (predicted.shape[0] * predicted.shape[1])\n",
    "        if debug:\n",
    "            print(f\"Sparsity: {sparsity}\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = DataPrep(data, idf=True, norm=None)\n",
    "train, seen, unseen = prep.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train, seen, unseen, idf, norm, batch_size=100):\n",
    "    ks = [5,10,25,50,100]\n",
    "    metrics = ['euclidean','cosine']\n",
    "\n",
    "    for k in ks:\n",
    "        for metric in metrics:\n",
    "            y_pred, y_true = batch_predict(train, seen, unseen, k=k, metric=metric, batch_size=batch_size)\n",
    "            output = score(y_pred, y_true, set_diff=False, debug=False)\n",
    "            output_str = f\"k: {k}, metric: {metric}, idf: {idf}, norm: {norm}\\n\\trmse: {output['rmse']}\\n\\tr^2: {output['r_squared']}\\n\\tsparsity: {output['sparsity']}\\n\"\n",
    "            with open(\"output.txt\", \"a\") as f:\n",
    "                f.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/326 [00:00<?, ?it/s]c:\\Users\\holcombea\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|██████████| 326/326 [15:00<00:00,  2.76s/it]\n",
      "  0%|          | 0/326 [00:00<?, ?it/s]c:\\Users\\holcombea\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|██████████| 326/326 [06:36<00:00,  1.22s/it]\n",
      "  0%|          | 0/326 [00:00<?, ?it/s]c:\\Users\\holcombea\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|██████████| 326/326 [18:21<00:00,  3.38s/it]\n",
      "  0%|          | 0/326 [00:00<?, ?it/s]c:\\Users\\holcombea\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|██████████| 326/326 [07:16<00:00,  1.34s/it]\n",
      "  0%|          | 0/326 [00:00<?, ?it/s]c:\\Users\\holcombea\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|██████████| 326/326 [24:32<00:00,  4.52s/it]\n",
      "100%|██████████| 326/326 [08:59<00:00,  1.66s/it]\n",
      "  0%|          | 0/326 [00:00<?, ?it/s]c:\\Users\\holcombea\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|██████████| 326/326 [30:58<00:00,  5.70s/it]\n",
      "100%|██████████| 326/326 [12:29<00:00,  2.30s/it]\n",
      "  0%|          | 0/326 [00:00<?, ?it/s]c:\\Users\\holcombea\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|██████████| 326/326 [36:21<00:00,  6.69s/it]\n",
      "100%|██████████| 326/326 [18:02<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "grid_search(train,seen,unseen,idf=True,norm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<130033x59047 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19995357 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred, y_true = batch_predict(train, seen, unseen, k=4, metric='euclidean', batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_output = score(y_pred, y_true, set_diff=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Results\n",
    "\n",
    "|Parameter Input| RMSE | $r^2$| Sparsity|\n",
    "|---------------|------|------|---------|\n",
    "|5, E, No-IDF   |3.093 |0.0   | 0.00212 |\n",
    "|5, C, No-IDF   |2.048 |0.0   | 0.00853 |\n",
    "|10,E, No-IDF   |2.897 |0.0   | 0.00349 |\n",
    "|10,C, No-IDF   |1.760 |0.0   | 0.0116  |\n",
    "|25,E, No-IDF   |2.640 |0.0   | 0.006   |\n",
    "|25,C, No-IDF   |1.456 |0.0   | 0.0176  |\n",
    "|50,E, No-IDF   |2.429 |0.0   | 0.0102  |\n",
    "|50,C, No-IDF   |1.282 |0.0   | 0.0243  |\n",
    "|100,E,No-IDF   |2.217 |0.0   | 0.0158  |\n",
    "|100,C,No-IDF   |1.154 |0.0   | 0.0335  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
