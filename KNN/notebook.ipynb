{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Reccomendation Algorithm\n",
    "\n",
    "I decided to use the PyNNDescent, which is based on the NN Descent algorithm. I decided to use an approximate nearest neighbors instead of true KNN as I calculated the time to find the predictions on the full data set to be around 2.5 hours. This wasn't feasable as lots of iterations needed to be ran. I used NN Descent as it is a scalable algorithm with relatively low overhead, and had a highly recommended python library to go along with it. I would have used HNSW, however the memory overhead had me worried as memory consumption was already a significant bottleneck.\n",
    "\n",
    "With NN Descent the total time to fit and predict on the full dataset is about 10 minutes, which is much more managable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from math import ceil, floor, sqrt\n",
    "from average import AverageRating\n",
    "from tqdm import tqdm\n",
    "from pynndescent.pynndescent_ import PyNNDescentTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample):\n",
    "    folder_path = \"C:\\\\Users\\\\holcombea\\\\grad\\\\Recommendation Systems\\\\movie-lens\"\n",
    "    ratings_path = folder_path + \"\\\\ratings.csv\"\n",
    "\n",
    "    if sample:\n",
    "        ratings_path = 'C:\\\\Users\\\\holcombea\\\\grad\\\\Recommendation Systems\\\\Average\\\\sampe_data.csv'\n",
    "        pass\n",
    "    ratings = pd.read_csv(ratings_path)\n",
    "    if not sample:\n",
    "        original_movie_ids = set(ratings['movieId'])\n",
    "        movie_id_map = {original: new for new, original in enumerate(original_movie_ids)}\n",
    "        ratings['movieId'] = ratings['movieId'].map(movie_id_map)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = load_data(sample=False)\n",
    "data = csr_matrix((ratings['rating'], (ratings['userId'], ratings['movieId'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162542, 59047)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNRating:\n",
    "    def __init__(self, k, metric: str):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.neighbors = PyNNDescentTransformer(n_neighbors=self.k, metric=self.metric)\n",
    "        self.training_ratings = None\n",
    "\n",
    "\n",
    "    def fit(self, training_ratings):\n",
    "        max_k = training_ratings.shape[0]\n",
    "        if self.k > max_k:\n",
    "            print(f\"k({self.k}) is greater than total samples, changing k to the number of samples({max_k})\")\n",
    "            self.k = max_k\n",
    "        self.neighbors = PyNNDescentTransformer(n_neighbors=self.k, metric=self.metric)\n",
    "        self.neighbors.fit(training_ratings)\n",
    "        self.training_ratings = training_ratings\n",
    "        \n",
    "\n",
    "    def predict(self, user_ratings):\n",
    "        if self.training_ratings is None:\n",
    "            raise Exception(\"Must fit before predicting\")\n",
    "        distances = self.neighbors.transform(user_ratings)\n",
    "        all_distances = distances.nonzero()[1]\n",
    "        neighbor_indices = np.array_split(all_distances, ceil(all_distances.shape[0]/self.k))\n",
    "        averageRating = AverageRating()\n",
    "        predictions = list()\n",
    "        for item in neighbor_indices:\n",
    "            averageRating.fit(self.training_ratings[item])\n",
    "            predictions.append(averageRating.predictions)\n",
    "        return vstack(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    \"\"\"\n",
    "    Class for splitting data and applying idf transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, train_size=0.8, idf=False, norm=None):\n",
    "        self.raw_data = data\n",
    "        self.train_size = train_size\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        self.seen = None\n",
    "        self.unseen = None\n",
    "        if idf:\n",
    "            idf_transformer = TfidfTransformer(norm=norm, use_idf=True)\n",
    "            self.raw_data = idf_transformer.fit_transform(self.raw_data)\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\"\n",
    "        Splits data into train and test. Also splits test data into seen and unseen groups.\n",
    "        \"\"\"\n",
    "        self.train, self.test = DataPrep.train_test_split(self.raw_data, train_size=self.train_size)\n",
    "        self.seen, self.unseen = DataPrep.user_split(self.test)\n",
    "        return self.train, self.seen, self.unseen\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_test_split(data, train_size):\n",
    "        train, test = sklearn_train_test_split(data, train_size=train_size)\n",
    "        return train, test\n",
    "\n",
    "    @staticmethod\n",
    "    def user_split(user_item_matrix: csr_matrix, split=0.8):\n",
    "        seen_data = np.array([])\n",
    "        seen_indices = np.array([])\n",
    "        seen_indptr = np.array([0])\n",
    "\n",
    "        unseen_data = np.array([])\n",
    "        unseen_indices = np.array([])\n",
    "        unseen_indptr = np.array([0])\n",
    "\n",
    "        for i in range(len(user_item_matrix.indptr.copy()) - 1):\n",
    "            row_start = user_item_matrix.indptr[i]\n",
    "            row_end   = user_item_matrix.indptr[i+1] \n",
    "            sample_size = floor((row_end - row_start) * split)\n",
    "            if sample_size == 0: #ensures something is in the test data\n",
    "                sample_size += 1\n",
    "\n",
    "            row_indices = user_item_matrix.indices[row_start: row_end]\n",
    "            row_data = user_item_matrix.data[row_start: row_end]\n",
    "\n",
    "\n",
    "            data_idx = np.arange(len(row_data))\n",
    "            seen_idx = np.random.choice(data_idx, size=sample_size, replace=False)\n",
    "            unseen_idx = np.setdiff1d(data_idx, seen_idx)\n",
    "            \n",
    "            #appending data to matrices\n",
    "            seen_data = np.append(seen_data, row_data[seen_idx])\n",
    "            unseen_data = np.append(unseen_data, row_data[unseen_idx])\n",
    "\n",
    "            #appending indices\n",
    "            seen_indices = np.append(seen_indices, row_indices[seen_idx])\n",
    "            unseen_indices = np.append(unseen_indices, row_indices[unseen_idx])\n",
    "\n",
    "            seen_indptr = np.append(seen_indptr, [seen_indptr[-1] + len(seen_idx)])\n",
    "            unseen_indptr = np.append(unseen_indptr, [unseen_indptr[-1] + len(unseen_idx)])\n",
    "        return csr_matrix((seen_data, seen_indices, seen_indptr), dtype=np.float32), csr_matrix((unseen_data, unseen_indices, unseen_indptr),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(train, seen, unseen, k, metric,batch_size):\n",
    "    model = KNNRating(k=k, metric=metric)\n",
    "    model.fit(train)\n",
    "\n",
    "    batched_input = list()\n",
    "    if batch_size > seen.shape[0]:\n",
    "        batched_input.append(seen)\n",
    "    else:\n",
    "        for i in range(0, seen.shape[0], batch_size):\n",
    "            batched_input.append(seen[i:i+batch_size])\n",
    "    predictions = list()\n",
    "    for batch in tqdm(batched_input):\n",
    "        predictions.append(model.predict(batch))\n",
    "    y_pred = vstack(predictions)\n",
    "    return y_pred, unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(predicted, true, rmse=True, pearson=True, sparsity=True, set_diff=True):\n",
    "    nonzeros = true.nonzero()\n",
    "    y_true = np.array(true[nonzeros], dtype=np.float32)[0]\n",
    "    y_pred = np.array(predicted[nonzeros], dtype=np.float32)[0]\n",
    "    if rmse:\n",
    "        print(f\"rmse: {sqrt(mean_squared_error(y_true, y_pred))}\")\n",
    "    if pearson:\n",
    "        print(f\"r2 score: {r2_score(y_true, y_pred)}\")\n",
    "    if set_diff:\n",
    "        #batching set diff\n",
    "        batch_size=400\n",
    "        pred_batched = list()\n",
    "        true_batched = list()\n",
    "        for i in range(0, true.shape[0], batch_size):\n",
    "            pred_batched.append(predicted[i:i+batch_size])\n",
    "            true_batched.append(true[i:i+batch_size])\n",
    "        pair_in_both = 0\n",
    "        pair_in_true = 0\n",
    "        for pred_batch, true_batch in tqdm(zip(pred_batched, true_batched)):\n",
    "            pred_users, pred_movies = pred_batch.nonzero()\n",
    "            pred_pairs = set(zip(pred_users,pred_movies))\n",
    "            true_users, true_movies = true_batch.nonzero()\n",
    "            true_pairs = set(zip(true_users, true_movies))\n",
    "\n",
    "            pair_in_both += len(true_pairs.intersection(pred_pairs))\n",
    "            pair_in_true += len(true_pairs)\n",
    "\n",
    "        print(f\"Fraction of user-movie pairs with non-zero predicted ratings: {pair_in_both / pair_in_true}\")\n",
    "    if sparsity:\n",
    "        print(f\"Sparsity: {predicted.getnnz() / (predicted.shape[0] * predicted.shape[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = DataPrep(data, idf=True, norm='l2')\n",
    "train, seen, unseen = prep.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = batch_predict(train, seen, unseen, k=50, metric='euclidean', batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.05422249715097562\n",
      "r2 score: -0.16656142397125384\n",
      "Sparsity: 0.012288553228225129\n"
     ]
    }
   ],
   "source": [
    "score(y_pred, y_true, set_diff=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Results\n",
    "\n",
    "|Parameter Input| RMSE| $r^2$| Sparsity|\n",
    "|--------------|------|------|---------|\n",
    "|10, E, IDF, l2|0.0587|-0.281|0.0134|\n",
    "|10, C, IDF, l2|0.0542|-0.167|0.0123|\n",
    "|50, E, IDF, l2||||"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
